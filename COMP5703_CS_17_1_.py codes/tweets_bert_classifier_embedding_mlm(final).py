# -*- coding: utf-8 -*-
"""Tweets_BERT_Classifier_Embedding_MLM(Final).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19yC9H35a2uaYeYemnO2qMgbvkYBLOqPS

## Library
"""

!pip install transformers
!pip install sentence_transformers
!pip install accelerate -U

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/classification

from transformers import AutoModelForSequenceClassification,Trainer,AutoTokenizer,TrainingArguments
import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
from torch.utils.data import Dataset
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score,f1_score,precision_score
from sklearn.model_selection import train_test_split

"""## Data pre-processing"""

dataset = "1"

if dataset == "1":

    #df = pd.read_excel("Constraint_Train.xlsx")
    df = pd.read_excel("./BERT/BERT/1/Constraint_Train.xlsx")
    test_df  = pd.read_excel("./BERT/BERT/1/Constraint_Test.xlsx")

elif dataset == "2":

    df = pd.read_excel("./BERT/BERT/2/texts_labeled_cleaned.xlsx")

    df.columns = ["tweet","label"]


    df,test_df = train_test_split(df,test_size=0.1,random_state=12)

elif dataset == "3":

    train = pd.read_csv('train.csv',delimiter='\t')
    test = pd.read_csv('test.csv',delimiter='\t')
    valid = pd.read_csv('valid.csv',delimiter='\t')

    replace_dict = {'half-true': 0, 'mostly-true': 0, 'false': 0, 'true': 1, 'barely-true': 0, 'pants-fire': 0}

    train['false'] = train['false'].replace(replace_dict)

    test['true'] = test['true'].replace(replace_dict)

    valid['barely-true'] = valid['barely-true'].replace(replace_dict)

    value_counts = train['false'].value_counts()

    label_train = train['false'].tolist()
    label_test = test['true'].tolist()
    train_set = train['Says the Annies List political group supports third-trimester abortions on demand.'].tolist()
    test_set = test['Building a wall on the U.S.-Mexico border will take literally years.'].tolist()
    unlabeled = valid['We have less Americans working now than in the 70s.'].tolist()
    df = pd.DataFrame({"tweet" : train_set + test_set,
              "label" : label_train + label_test })

    test_df = pd.DataFrame({"tweet" : unlabeled} )




class TextDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

encoder     = LabelEncoder()
df['label'] = encoder.fit_transform( df['label'] )
# test_df['label'] = encoder.transform(test_df['label'])

import numpy as np

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    return {"accuracy" : accuracy_score(labels,predictions),
            "f1"  : f1_score(labels,predictions),
            "precision"  : precision_score(labels,predictions)}

"""## Sanity Test"""

from transformers import BertTokenizer, BertModel
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
# 初始化 BERT 模型和分词器
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')
# 定义获取 BERT 嵌入的函数
def get_bert_embeddings(texts):
    embeddings = []
    for text in texts:
        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)
        with torch.no_grad():
            outputs = model(**inputs)
        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy())
    return np.array(embeddings)

# 定义一小批示例文本和标签
texts = [
    # COVID-related sentences
    "The COVID-19 vaccine rollout is happening.",
    "COVID cases are spiking in my area.",
    "Due to the pandemic, the event was cancelled.",
    "What are the symptoms of the coronavirus?",
    "People are stockpiling hand sanitizers due to COVID.",
    "COVID-19 has impacted the global economy.",
    "Social distancing is key to stop the spread.",
    "Schools have been closed due to COVID-19.",
    "Many people have lost their jobs due to the pandemic.",
    "Healthcare workers are on the frontline.",

    # Non-COVID-related sentences
    "I like to play video games.",
    "She is going on a vacation to the mountains.",
    "I bought a new pair of shoes.",
    "The book I read was fantastic.",
    "I love Italian food.",
    "The movie was not that great.",
    "I like to listen to classical music.",
    "I bought a new phone.",
    "I love to travel and see new places.",
    "My dog is really cute.",
]

# Labels for these texts
labels = [1]*10 + [0]*10  # First 10 are COVID-related (1), next 10 are not (0)

# 获取 BERT 嵌入
embeddings = get_bert_embeddings(texts)

# 使用 t-SNE 进行降维
tsne = TSNE(n_components=2, perplexity=10, random_state=0)
embeddings_2d = tsne.fit_transform(embeddings)

# 进行可视化
plt.figure(figsize=(12, 8))

for i, label in enumerate(labels):
    x, y = embeddings_2d[i]
    if label == 1:
        plt.scatter(x, y, c='r')
        plt.text(x, y, f"Text-{i} (COVID-related)", fontsize=12)
    else:
        plt.scatter(x, y, c='b')
        plt.text(x, y, f"Text-{i} (Non-COVID)", fontsize=12)

plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.title('BERT Embeddings Visualization')
plt.show()

"""## BERT classifier"""

model_name = "bert-base-uncased"

tokenizer = AutoTokenizer.from_pretrained(model_name)

id2label = {0 : 0 ,1 : 1 }
label2id = {0 : 0, 1 : 1 }

model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=2, id2label=id2label, label2id=label2id)

train_df,val_df = train_test_split(df,test_size=0.2,random_state=12)

train_encoding =  tokenizer(train_df['tweet'].values.tolist(), truncation=True, padding=True)
val_encoding   = tokenizer(val_df['tweet'].values.tolist(), truncation=True, padding=True)
train_dataset = TextDataset(train_encoding,train_df['label'].values.tolist())
val_dataset   = TextDataset(val_encoding,val_df['label'].values.tolist())

training_args = TrainingArguments(
    output_dir=model_name,
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=False,
)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()

"""## Embedding (original model）"""

from sentence_transformers import SentenceTransformer
model_name = "bert-base-uncased"
model = SentenceTransformer(model_name)

train_embedding = model.encode( train_df["tweet"].values.tolist() )
val_embedding   = model.encode( val_df["tweet"].values.tolist() )

def compute_metrics(model_name,predict,true):
    return {
        "model_name" : model_name,
        "accuracy" : accuracy_score(true,predict),
        "f1" : f1_score(true,predict),
        "precision" : precision_score(true,predict)
    }

train_embedding.shape

val_embedding.shape

train_label = train_df["label"].tolist()
val_label   = val_df["label"].tolist()

"""## classical classifier"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score

models = [KNeighborsClassifier,LogisticRegression,DecisionTreeClassifier,MLPClassifier]
result = []
for m in models:
    model = m()
    model.fit(train_embedding,
              train_label)

    val_predict = model.predict(val_embedding)

    result.append(compute_metrics(m.__name__,
                                  val_predict,
                                  val_label))

print("dataset:",dataset)

pd.DataFrame(result)

"""## KNN method"""

from tqdm.auto import tqdm

Ks = [1,2,3,4,5,6,7,8,9,10,20,30]

result = []

for k in tqdm(Ks):
    model = KNeighborsClassifier(n_neighbors=k)

    model.fit(train_embedding,train_label)

    val_predict = model.predict(val_embedding)

    result.append(compute_metrics("{}".format(k),
                                  val_predict,
                                  val_label))

knndf = pd.DataFrame(result)

knndf.columns = ["K","accuracy","f1","precision"]
knndf.index = knndf.K

knndf = knndf.drop(columns = ["K"])
knndf

import matplotlib.pyplot as plt
plt.style.use("seaborn-v0_8-whitegrid")

fig,ax = plt.subplots(1,1,figsize=(10,6))
knndf.plot(ax = ax)

"""## CNN realization"""

X_train = torch.tensor(train_embedding, dtype=torch.float32)
y_train = torch.tensor(train_label, dtype=torch.float32).unsqueeze(1)
X_val = torch.tensor(val_embedding, dtype=torch.float32)
y_val = torch.tensor(val_label, dtype=torch.float32).unsqueeze(1)

class FakeNewsClassifierCNN(nn.Module):
    def __init__(self):
        super(FakeNewsClassifierCNN, self).__init__()

        # First convolutional layer
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)  # Reduce dimension by half: 16x12

        # Second convolutional layer
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)  # Reduce dimension to: 8x6

        # Fully connected layers
        self.fc1 = nn.Linear(64 * 8 * 6, 128)
        self.bn3 = nn.BatchNorm1d(128)
        self.fc2 = nn.Linear(128, 1)

        # Dropout layer
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        # Reshape input for CNN: (batch_size, 1, 32, 24)
        x = x.view(-1, 1, 32, 24)

        # First conv block
        x = self.pool1(F.relu(self.bn1(self.conv1(x))))
        x = self.dropout(x)

        # Second conv block
        x = self.pool2(F.relu(self.bn2(self.conv2(x))))
        x = self.dropout(x)

        # Flatten the output
        x = x.view(x.size(0), -1)

        # Fully connected layers
        x = F.relu(self.bn3(self.fc1(x)))
        x = self.dropout(x)
        x = torch.sigmoid(self.fc2(x))

        return x

from torch.utils.data import DataLoader, TensorDataset

# Create datasets
train_dataset = TensorDataset(X_train, y_train)
val_dataset = TensorDataset(X_val, y_val)

# Define data loaders
batch_size = 32  # You can change the batch size as needed
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

import torch
import torch.nn as nn
import numpy as np
import random

# 设置随机种子以获得可重复的结果
seed = 10
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
torch.backends.cudnn.deterministic = True  # 注意：这可能会稍微降低运行速度
torch.backends.cudnn.benchmark = False
np.random.seed(seed)
random.seed(seed)

# 检查是否有可用的GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 初始化模型
model = FakeNewsClassifierCNN().to(device)  # 确保模型在GPU上

# 定义损失和优化器
criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification
optimizer = torch.optim.Adam(model.parameters(), lr=0.005)

# 训练循环
num_epochs = 200  # 您可以根据需要更改epochs的数量

best_accuracy = 0  # 保存最高准确率

for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    # 训练阶段
    for batch_x, batch_y in train_loader:
        batch_x, batch_y = batch_x.to(device), batch_y.to(device)  # 数据移至GPU

        optimizer.zero_grad()

        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}")

    # 验证阶段
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for batch_x, batch_y in val_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)  # 数据移至GPU
            outputs = model(batch_x)
            predicted = (outputs > 0.5).float()
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()

    current_accuracy = correct/total*100
    print(f"Accuracy on validation data: {current_accuracy:.2f}%")

    # 如果当前准确率高于迄今为止的最高准确率，则保存此准确率
    if current_accuracy > best_accuracy:
        best_accuracy = current_accuracy

print(f"Highest Accuracy on validation data: {best_accuracy:.2f}%")

X_train = torch.tensor(train_embedding, dtype=torch.float32)
y_train = torch.tensor(train_label, dtype=torch.long)
X_val = torch.tensor(val_embedding, dtype=torch.float32)
y_val = torch.tensor(val_label, dtype=torch.long)

class MLPClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MLPClassifier, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            # you might have more layers here
            nn.Linear(hidden_dim, output_dim)  # This should ensure the final output has shape (N, 2)
        )

    def forward(self, x):
        x = self.layers(x)
        return x  # No softmax here, CrossEntropyLoss expects raw scores (logits)

from torch.utils.data import DataLoader, TensorDataset
import torch.optim as optim

# Create datasets
train_dataset = TensorDataset(X_train, y_train)
val_dataset = TensorDataset(X_val, y_val)

# Define data loaders
batch_size = 32  # You can change the batch size as needed
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# 设置随机种子以获得可重复的结果
seed = 10
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
torch.backends.cudnn.deterministic = True  # 注意：这可能会稍微降低运行速度
torch.backends.cudnn.benchmark = False
np.random.seed(seed)
random.seed(seed)

# 设置设备
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 参数设置
input_dim = 768
hidden_dim = 256
output_dim = 2  # 二分类

# 实例化模型并移至设备
model = MLPClassifier(input_dim, hidden_dim, output_dim).to(device)

# 定义损失函数和优化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

def train_model(num_epochs, save_best_model=False):
    model.train()

    best_val_accuracy = 0  # 追踪最高的验证准确率
    for epoch in range(num_epochs):
        total_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}")

        # 在每个epoch结束后，都用验证集来评估模型
        current_val_accuracy = validate_model()
        if current_val_accuracy > best_val_accuracy:
            best_val_accuracy = current_val_accuracy
            if save_best_model:
                # 如果这个epoch的模型在验证集上达到了新的最佳表现，就保存模型
                torch.save(model.state_dict(), 'best_model.pth')
                print(f"Detected model improvement, saving current model to 'best_model.pth'")

    print(f'Best Validation Accuracy: {best_val_accuracy:.2f}%')


def validate_model():
    model.eval()
    correct_predictions = 0
    total_predictions = 0
    with torch.no_grad():
        for batch_x, batch_y in val_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            outputs = model(batch_x)
            _, predicted = torch.max(outputs, 1)
            correct_predictions += (predicted == batch_y).sum().item()
            total_predictions += batch_y.size(0)

    accuracy = 100 * correct_predictions / total_predictions
    return accuracy  # 现在，这个函数返回当前的准确率

# 你可以选择是否在每个epoch后保存最佳模型
num_epochs = 200
train_model(num_epochs, save_best_model=True)

"""## MLM fine-tuning (self-supervision)"""

# Load the Constraint_Test dataset
constraint_test_df = test_df


# Use the tweet column for fine-tuning
unlabeled_texts = constraint_test_df["tweet"].tolist()

from transformers import BertTokenizer, BertForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments
from torch.utils.data import Dataset

# Load the pre-trained model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')

# Tokenize the texts
encodings = tokenizer(unlabeled_texts, truncation=True, padding='max_length', max_length=512, return_tensors='pt')

# Custom dataset class for MLM
class MLM_Dataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = item['input_ids'].detach().clone()
        return item

    def __len__(self):
        return len(self.encodings['input_ids'])

# Create a DataLoader using the encodings
dataset = MLM_Dataset(encodings)
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)

# Training arguments and Trainer
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=1,
    per_device_train_batch_size=8,
    logging_dir='./logs',
)
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

# Fine-tune the model using the unlabeled data
trainer.train()

# Save the fine-tuned model
model_directory = "/content/drive/MyDrive/classification/BERT/BERT/1/fine_tuned_model"
model.save_pretrained(model_directory)

# Save the tokenizer
tokenizer.save_pretrained(model_directory)

"""## Embedding methods (using MLM fine-tuned models)"""

from sentence_transformers import SentenceTransformer, models
# 加载微调后的模型
transformers_model_path = "/content/drive/MyDrive/classification/BERT/BERT/1/fine_tuned_model"

# 创建sentence-transformers模型
word_embedding_model = models.Transformer(transformers_model_path, model_args={'output_hidden_states': True})
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())
sentence_transformer_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])

# Obtain embeddings for train and validation sets
train_embedding_finetuned = sentence_transformer_model.encode(train_df["tweet"].values.tolist())
val_embedding_finetuned = sentence_transformer_model.encode(val_df["tweet"].values.tolist())

# Define labels
train_label = train_df["label"].tolist()
val_label = val_df["label"].tolist()

from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score

def compute_metrics(model_name,predict,true):
    return {
        "model_name" : model_name,
        "accuracy" : accuracy_score(true,predict),
        "f1" : f1_score(true,predict),
        "precision" : precision_score(true,predict)
    }

models = [KNeighborsClassifier,LogisticRegression,DecisionTreeClassifier,MLPClassifier]
result = []
for m in models:
    model = m()
    model.fit(train_embedding_finetuned,
              train_label)

    val_predict = model.predict(val_embedding_finetuned)

    result.append(compute_metrics(m.__name__,
                                  val_predict,
                                  val_label))

print("dataset:",dataset)

pd.DataFrame(result)

from tqdm.auto import tqdm

Ks = [1,2,3,4,5,6,7,8,9,10,20,30]

result = []

for k in tqdm(Ks):
    model = KNeighborsClassifier(n_neighbors=k)

    model.fit(train_embedding_finetuned,train_label)

    val_predict = model.predict(val_embedding_finetuned)

    result.append(compute_metrics("{}".format(k),
                                  val_predict,
                                  val_label))

knndf = pd.DataFrame(result)

knndf.columns = ["K","accuracy","f1","precision"]
knndf.index = knndf.K

knndf = knndf.drop(columns = ["K"])
knndf

import matplotlib.pyplot as plt
plt.style.use("seaborn-v0_8-whitegrid")

fig,ax = plt.subplots(1,1,figsize=(10,6))
knndf.plot(ax = ax)

X_train = torch.tensor(train_embedding_finetuned, dtype=torch.float32)
y_train = torch.tensor(train_label, dtype=torch.float32).unsqueeze(1)
X_val = torch.tensor(val_embedding_finetuned, dtype=torch.float32)
y_val = torch.tensor(val_label, dtype=torch.float32).unsqueeze(1)

class FakeNewsClassifierCNN(nn.Module):
    def __init__(self):
        super(FakeNewsClassifierCNN, self).__init__()

        # First convolutional layer
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)  # Reduce dimension by half: 16x12

        # Second convolutional layer
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)  # Reduce dimension to: 8x6

        # Fully connected layers
        self.fc1 = nn.Linear(64 * 8 * 6, 128)
        self.bn3 = nn.BatchNorm1d(128)
        self.fc2 = nn.Linear(128, 1)

        # Dropout layer
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        # Reshape input for CNN: (batch_size, 1, 32, 24)
        x = x.view(-1, 1, 32, 24)

        # First conv block
        x = self.pool1(F.relu(self.bn1(self.conv1(x))))
        x = self.dropout(x)

        # Second conv block
        x = self.pool2(F.relu(self.bn2(self.conv2(x))))
        x = self.dropout(x)

        # Flatten the output
        x = x.view(x.size(0), -1)

        # Fully connected layers
        x = F.relu(self.bn3(self.fc1(x)))
        x = self.dropout(x)
        x = torch.sigmoid(self.fc2(x))

        return x

from torch.utils.data import DataLoader, TensorDataset

# Create datasets
train_dataset = TensorDataset(X_train, y_train)
val_dataset = TensorDataset(X_val, y_val)

# Define data loaders
batch_size = 32  # You can change the batch size as needed
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

import torch
import torch.nn as nn
import numpy as np
import random

# 设置随机种子以获得可重复的结果
seed = 10
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
torch.backends.cudnn.deterministic = True  # 注意：这可能会稍微降低运行速度
torch.backends.cudnn.benchmark = False
np.random.seed(seed)
random.seed(seed)

# 检查是否有可用的GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 初始化模型
model = FakeNewsClassifierCNN().to(device)  # 确保模型在GPU上

# 定义损失和优化器
criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification
optimizer = torch.optim.Adam(model.parameters(), lr=0.005)

# 训练循环
num_epochs = 200  # 您可以根据需要更改epochs的数量

best_accuracy = 0  # 保存最高准确率

for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    # 训练阶段
    for batch_x, batch_y in train_loader:
        batch_x, batch_y = batch_x.to(device), batch_y.to(device)  # 数据移至GPU

        optimizer.zero_grad()

        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}")

    # 验证阶段
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for batch_x, batch_y in val_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)  # 数据移至GPU
            outputs = model(batch_x)
            predicted = (outputs > 0.5).float()
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()

    current_accuracy = correct/total*100
    print(f"Accuracy on validation data: {current_accuracy:.2f}%")

    # 如果当前准确率高于迄今为止的最高准确率，则保存此准确率
    if current_accuracy > best_accuracy:
        best_accuracy = current_accuracy

print(f"Highest Accuracy on validation data: {best_accuracy:.2f}%")

X_train = torch.tensor(train_embedding_finetuned, dtype=torch.float32)
y_train = torch.tensor(train_label, dtype=torch.long)
X_val = torch.tensor(val_embedding_finetuned, dtype=torch.float32)
y_val = torch.tensor(val_label, dtype=torch.long)

class MLPClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MLPClassifier, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            # you might have more layers here
            nn.Linear(hidden_dim, output_dim)  # This should ensure the final output has shape (N, 2)
        )

    def forward(self, x):
        x = self.layers(x)
        return x  # No softmax here, CrossEntropyLoss expects raw scores (logits)

from torch.utils.data import DataLoader, TensorDataset
import torch.optim as optim

# Create datasets
train_dataset = TensorDataset(X_train, y_train)
val_dataset = TensorDataset(X_val, y_val)

# Define data loaders
batch_size = 32  # You can change the batch size as needed
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# 设置随机种子以获得可重复的结果
seed = 10
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
torch.backends.cudnn.deterministic = True  # 注意：这可能会稍微降低运行速度
torch.backends.cudnn.benchmark = False
np.random.seed(seed)
random.seed(seed)

# 设置设备
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 参数设置
input_dim = 768
hidden_dim = 256
output_dim = 2  # 二分类

# 实例化模型并移至设备
model = MLPClassifier(input_dim, hidden_dim, output_dim).to(device)

# 定义损失函数和优化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

def train_model(num_epochs, save_best_model=False):
    model.train()

    best_val_accuracy = 0  # 追踪最高的验证准确率
    for epoch in range(num_epochs):
        total_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}")

        # 在每个epoch结束后，都用验证集来评估模型
        current_val_accuracy = validate_model()
        if current_val_accuracy > best_val_accuracy:
            best_val_accuracy = current_val_accuracy
            if save_best_model:
                # 如果这个epoch的模型在验证集上达到了新的最佳表现，就保存模型
                torch.save(model.state_dict(), 'best_model.pth')
                print(f"Detected model improvement, saving current model to 'best_model.pth'")

    print(f'Best Validation Accuracy: {best_val_accuracy:.2f}%')


def validate_model():
    model.eval()
    correct_predictions = 0
    total_predictions = 0
    with torch.no_grad():
        for batch_x, batch_y in val_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            outputs = model(batch_x)
            _, predicted = torch.max(outputs, 1)
            correct_predictions += (predicted == batch_y).sum().item()
            total_predictions += batch_y.size(0)

    accuracy = 100 * correct_predictions / total_predictions
    return accuracy  # 现在，这个函数返回当前的准确率

# 你可以选择是否在每个epoch后保存最佳模型
num_epochs = 200
train_model(num_epochs, save_best_model=True)