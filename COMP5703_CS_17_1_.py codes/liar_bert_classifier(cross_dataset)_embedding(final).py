# -*- coding: utf-8 -*-
"""LIAR_BERT_Classifier(cross_dataset)_Embedding(Final).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QeX39Xjq5FZSkFzdw3I59JnA_B5hKX3X

## Library
"""

!pip install transformers
!pip install sentence_transformers
!pip install accelerate -U

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/classification

from transformers import AutoModelForSequenceClassification,Trainer,AutoTokenizer,TrainingArguments
import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
from torch.utils.data import Dataset
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score,f1_score,precision_score
from sklearn.model_selection import train_test_split

"""## Data pre-processing"""

#Importing into the database, 'dataset = 1' represents the tweet dataset, 2 stands for the Kaggle dataset, and 3 for the liar dataset

dataset = "3"

if dataset == "1":

    df = pd.read_excel("Constraint_Train.xlsx")
    test_df  = pd.read_excel("Constraint_Test.xlsx")

elif dataset == "2":

    #df = pd.read_excel("texts_labeled_cleaned.xlsx")
    df = pd.read_excel("./BERT/BERT/2/texts_labeled_cleaned.xlsx")

    df.columns = ["tweet","label"]


    df,test_df = train_test_split(df,test_size=0.1,random_state=12)

elif dataset == "3":

    train = pd.read_csv('./BERT/BERT/3/train.csv',delimiter='\t')
    test = pd.read_csv('./BERT/BERT/3/test.csv',delimiter='\t')
    valid = pd.read_csv('./BERT/BERT/3/valid.csv',delimiter='\t')
    #train = pd.read_csv('train.csv',delimiter='\t')
    #test = pd.read_csv('test.csv',delimiter='\t')
    #valid = pd.read_csv('valid.csv',delimiter='\t')

    replace_dict = {'half-true': 0, 'mostly-true': 0, 'false': 0, 'true': 1, 'barely-true': 0, 'pants-fire': 0}

    train['false'] = train['false'].replace(replace_dict)

    test['true'] = test['true'].replace(replace_dict)

    valid['barely-true'] = valid['barely-true'].replace(replace_dict)

    value_counts = train['false'].value_counts()

    label_train = train['false'].tolist()
    label_test = test['true'].tolist()
    train_set = train['Says the Annies List political group supports third-trimester abortions on demand.'].tolist()
    test_set = test['Building a wall on the U.S.-Mexico border will take literally years.'].tolist()
    unlabeled = valid['We have less Americans working now than in the 70s.'].tolist()
    df = pd.DataFrame({"tweet" : train_set + test_set,
              "label" : label_train + label_test })

    test_df = pd.DataFrame({"tweet" : unlabeled} )


class TextDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

encoder = LabelEncoder()
df['label'] = encoder.fit_transform( df['label'] )

import numpy as np

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    return {"accuracy" : accuracy_score(labels,predictions),
            "f1"  : f1_score(labels,predictions),
            "precision"  : precision_score(labels,predictions)}

"""## BERT classifier"""

model_name = "bert-base-uncased"

tokenizer = AutoTokenizer.from_pretrained(model_name)

id2label = {0 : 0 ,1 : 1}
label2id = {0 : 0, 1 : 1}

model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=2, id2label=id2label, label2id=label2id)

train_df,val_df = train_test_split(df,test_size=0.2,random_state=12)

train_encoding =  tokenizer(train_df['tweet'].values.tolist(), truncation=True, padding=True)
val_encoding   = tokenizer(val_df['tweet'].values.tolist(), truncation=True, padding=True)
train_dataset = TextDataset(train_encoding,train_df['label'].values.tolist())
val_dataset   = TextDataset(val_encoding,val_df['label'].values.tolist())

training_args = TrainingArguments(
    output_dir=model_name,
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=False,
)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()

"""## Cross dataset testing"""

class TestDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings
        # self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        # item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        keys = self.encodings.keys()
        keys = list(keys)

        return len(self.encodings[keys[0]])

df = pd.read_excel("./BERT/BERT/1/Constraint_Train.xlsx")
encoding =  tokenizer(df['tweet'].values.tolist(), truncation=True, padding=True)
labels  =  LabelEncoder().fit_transform( df["label"].values )

test_dataset=TestDataset(encoding)
trainer.model.eval()
test_predict = trainer.predict(test_dataset)
# trainer.predict(test_dataset)

np.mean(test_predict.predictions.argmax(axis = 1) == labels)

"""## Embedding"""

from sentence_transformers import SentenceTransformer
model = SentenceTransformer(model_name)

train_embedding = model.encode(train_df["tweet"].values.tolist())
val_embedding   = model.encode(val_df["tweet"].values.tolist())

train_embedding.shape

val_embedding.shape

train_label = train_df["label"].tolist()
val_label   = val_df["label"].tolist()

"""## classical classifier"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score

def compute_metrics(model_name,predict,true):
    return {
        "model_name" : model_name,
        "accuracy" : accuracy_score(true,predict),
        "f1" : f1_score(true,predict),
        "precision" : precision_score(true,predict)
    }

models = [KNeighborsClassifier,LogisticRegression,DecisionTreeClassifier,MLPClassifier]
result = []
for m in models:
    model = m()
    model.fit(train_embedding,
              train_label)

    val_predict = model.predict(val_embedding)

    result.append(compute_metrics(m.__name__,
                                  val_predict,
                                  val_label))

print("dataset:",dataset)

pd.DataFrame(result)

"""## KNN method"""

from tqdm.auto import tqdm

Ks = [1,2,3,4,5,6,7,8,9,10,20,30]

result = []

for k in tqdm(Ks):
    model = KNeighborsClassifier(n_neighbors=k)

    model.fit(train_embedding,train_label)

    val_predict = model.predict(val_embedding)

    result.append(compute_metrics("{}".format(k),
                                  val_predict,
                                  val_label))

knndf = pd.DataFrame(result)

knndf.columns = ["K","accuracy","f1","precision"]
knndf.index = knndf.K

knndf = knndf.drop(columns = ["K"])
knndf

import matplotlib.pyplot as plt
plt.style.use("seaborn-v0_8-whitegrid")

fig,ax = plt.subplots(1,1,figsize=(10,6))
knndf.plot(ax = ax)

"""## MLP realization"""

from torch.utils.data import DataLoader, TensorDataset
import torch.nn as nn
import numpy as np
import random
import torch.optim as optim

X_train = torch.tensor(train_embedding, dtype=torch.float32)
y_train = torch.tensor(train_label, dtype=torch.long)
X_val = torch.tensor(val_embedding, dtype=torch.float32)
y_val = torch.tensor(val_label, dtype=torch.long)

# Create datasets
train_dataset = TensorDataset(X_train, y_train)
val_dataset = TensorDataset(X_val, y_val)

# Define data loaders
batch_size = 32  # You can change the batch size as needed
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

class BinaryMLPClassifier(nn.Module):
    def __init__(self, input_dim=768, hidden_layer_sizes=[512, 256], dropout_rate=0.5):
        super(BinaryMLPClassifier, self).__init__()

        # Initialize layers
        self.layers = nn.Sequential()

        # Input layer
        current_input_dim = input_dim

        # Add hidden layers
        for i, hidden_size in enumerate(hidden_layer_sizes):
            self.layers.add_module('linear{}'.format(i), nn.Linear(current_input_dim, hidden_size))
            self.layers.add_module('relu{}'.format(i), nn.ReLU())
            self.layers.add_module('dropout{}'.format(i), nn.Dropout(dropout_rate))
            current_input_dim = hidden_size  # Update input dim for the next layer

        # Output layer
        self.layers.add_module('output', nn.Linear(current_input_dim, 1))


    def forward(self, x):
        return self.layers(x)  # No sigmoid, assuming BCEWithLogitsLoss will be used

# Set random seed for reproducibility
def set_seed(seed_value):
    torch.manual_seed(seed_value)
    torch.cuda.manual_seed_all(seed_value)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    np.random.seed(seed_value)

    random.seed(seed_value)

# Random seed set to some fixed number
set_seed(42)

# Initialize your model, optimizer, and loss function
model = BinaryMLPClassifier()
# optimizer = optim.Adam(model.parameters(), lr=0.001)
# optimizer = torch.optim.RMSprop(model.parameters(), lr=0.005, weight_decay=1e-4)

optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)
criterion = nn.BCEWithLogitsLoss()

# Track the best accuracy
best_accuracy = 0.0
best_epoch = 0

# Training loop
num_epochs = 100
for epoch in range(num_epochs):
    model.train()  # Set the model to training mode
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        outputs = outputs.squeeze(1)
        labels = labels.float()
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    # Validation loop
    model.eval()  # Set the model to evaluation mode
    all_predictions = []
    all_targets = []
    with torch.no_grad():
        for inputs, labels in val_loader:
            outputs = model(inputs)
            predictions = torch.round(torch.sigmoid(outputs))
            all_predictions.extend(predictions.cpu().numpy())
            all_targets.extend(labels.cpu().numpy())

    # Calculate accuracy
    accuracy = accuracy_score(all_targets, all_predictions)
    print(f'Epoch {epoch + 1}/{num_epochs} - Validation Accuracy: {accuracy:.4f}')

    # Save the best model
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_epoch = epoch
        torch.save(model.state_dict(), 'best_model.pth')
        print(f'New best model saved at epoch {epoch + 1} with accuracy {best_accuracy:.4f}')

print(f'Best Accuracy: {best_accuracy:.4f} at epoch {best_epoch + 1}')